<!DOCTYPE html>
<html lang="en-zh">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title> spark学习笔记-RDD基础算子 | Hugo Prose</title>
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/css/article.min.css,npm/@xiee/utils/css/heading-anchor.min.css">
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="spark学习笔记-RDD基础算子"/>
<meta name="twitter:description" content="摘要：学习spark过程中的笔记，记录spark中的基础算子，以及RDD的基本概念。
spark transform operation 源码地址：https://github.com/YaoQi17/sparkLearning/tree/master/sparkRDD
总结 RDD(Resilient Distributed Dataset) 弹性分布式数据集，是一组分布式的数据集合，里面的元素可并行计算，可分区； RDD允许用户在执行多个查询时显示地将工作集缓存在内存中，例如persist()；
创建方式 创建RDD的两种方式：
读取外界文件 外界文件不局限于系统文件，包括HDFS、HBase等
sparkSession.sparkContext.textFile(&quot;sparkRDD/src/main/resources/data.txt&quot;) 通过并行化的方式创建 val sparkSession = getDefaultSparkSession val dataArray = Array(1, 2, 3, 4, 5, 6) // 创建一个RDD val rdd = sparkSession.sparkContext.parallelize(dataArray) 通过并行化的方式创建还可以指定分区的数量
/** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection."/>


  </head>

  <body>

    <nav class="menu">
    <ul>
      <li class="left">
        <a href="/"><span>Hugo Prose</span></a>
      </li>
      
      <li id="menu-search">
        <a href="/#">Search</a>
      </li>
      
      <li>
        <a href="/index.xml">Subscribe</a>
      </li>
      
    </ul>
    </nav>


<div class="container single">
<main>

<div class="article-meta">
<h1><span class="title">spark学习笔记-RDD基础算子</span></h1>

<h3 class="date">2019-07-27</h3>
<p class="terms">
  
  
  
  
  Tags: <a href="/tags/spark">Spark</a> 
  
  
</p>
</div>

<div class="article">
<p>摘要：学习spark过程中的笔记，记录spark中的基础算子，以及RDD的基本概念。</p>
<hr>
<h2 id="spark-transform-operation">spark transform operation</h2>
<p>源码地址：<a href="https://github.com/YaoQi17/sparkLearning/tree/master/sparkRDD">https://github.com/YaoQi17/sparkLearning/tree/master/sparkRDD</a></p>
<hr>
<h2 id="总结">总结</h2>
<p>RDD(Resilient Distributed Dataset) 弹性分布式数据集，是一组分布式的数据集合，里面的元素可并行计算，可分区；
RDD允许用户在执行多个查询时显示地将工作集缓存在内存中，例如persist()；</p>
<h3 id="创建方式">创建方式</h3>
<p>创建RDD的两种方式：</p>
<ul>
<li>读取外界文件</li>
</ul>
<p>外界文件不局限于系统文件，包括HDFS、HBase等</p>
<pre><code class="language-scala">sparkSession.sparkContext.textFile(&quot;sparkRDD/src/main/resources/data.txt&quot;)
</code></pre>
<ul>
<li>通过并行化的方式创建</li>
</ul>
<pre><code class="language-scala">val sparkSession = getDefaultSparkSession
val dataArray = Array(1, 2, 3, 4, 5, 6)
// 创建一个RDD
val rdd = sparkSession.sparkContext.parallelize(dataArray)
</code></pre>
<p>通过并行化的方式创建还可以指定分区的数量</p>
<pre><code class="language-scala">/** Distribute a local Scala collection to form an RDD.
   *
   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call
   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the
   * modified collection. Pass a copy of the argument to avoid this.
   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an
   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.
   * @param seq Scala collection to distribute
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed collection
   */
  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    assertNotStopped()
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }
</code></pre>
<h3 id="rdd编程">RDD编程</h3>
<p>RDD中包含两种类型的算子：Transformation和Action；</p>
<h4 id="transformation-算子">Transformation 算子</h4>
<p>Transformation 转换操作，将一个RDD转化为另外一个RDD，但是该过程具有延迟加载性；
Transformation算子不会马上执行，只有当遇到Action算子时才会执行。</p>
<p>常见的Transformation算子：</p>
<ul>
<li>
<p><strong>map(function)</strong> 由一个RDD转化为另外一个RDD，function的每一次输出组成另外一个RDD；</p>
</li>
<li>
<p><strong>filter(function)</strong> 由一个RDD的元素经过筛选，满足function条件的元素组成一个新的RDD；</p>
</li>
<li>
<p><strong>flatMap(function)</strong> 类似于map，但是每一个元素可以被转化为多个元素，function应该返回一个序列；</p>
</li>
</ul>
<pre><code class="language-scala">  /**
    * flatMap实例，flatMap返回的是一组元素，官网说是一个Seq ，而不是一个元素
    */
  def flatMapDemo(): Unit = {
    val sparkSession = getDefaultSparkSession
    val textData = sparkSession.sparkContext.textFile(&quot;sparkRDD/src/main/resources/data.txt&quot;)
    //    val flatData = textData.flatMap(row =&gt; row.split(&quot; &quot;))
    val flatData = textData.map(row =&gt; row.split(&quot; &quot;))
    flatData.collect().foreach(println(_))
  }
</code></pre>
<ul>
<li>
<p><strong>mapPartitions(function)</strong> 类似于map，但独立地在RDD的每一个分片上运行，函数类型是：Iterator[T] =&gt; Iterator[U]；</p>
<p>传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部，在函数外部求值；</p>
<p>传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部，在函数内部求值；</p>
<p>Iterator[T] =&gt; Iterator[U] 就是表示该函数为传名调用。</p>
</li>
<li>
<p><strong>mapPartitionsWithIndex(function)</strong> 类似于mapPartitions，但是传入的参数中多了一个索引值，该索引值为RDD分片数的索引值；
（传入的函数类型为：(Int, Iterator<T>) =&gt; Iterator<U>）</p>
</li>
</ul>
<pre><code class="language-scala">  /**
    * mapPartitionsWithIndex使用示例
    */
  def mapPartitionsWithIndexDemo(): Unit = {
    val sparkSession = getDefaultSparkSession
    val rddData = sparkSession.sparkContext.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;))
    val data = rddData.mapPartitionsWithIndex((index: Int, row: Iterator[String]) =&gt; {
      row.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;:&quot; + x + &quot;]&quot;).iterator
    })
    data.collect().foreach(println(_))
  }

</code></pre>
<ul>
<li>
<p><strong>sample(withReplacement, fraction, seed)</strong> 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子；</p>
</li>
<li>
<p><strong>union(otherDataset)</strong>  对源RDD和参数中的RDD求并集后返回一个新的RDD；</p>
</li>
</ul>
<pre><code class="language-scala">  /**
    * 求并集示例
    */
  def unionRDD(): Unit = {
    val sparkSession = getDefaultSparkSession
    val rddData1 = sparkSession.sparkContext.parallelize(List(&quot;s&quot;, &quot;d&quot;, &quot;f&quot;, &quot;a&quot;))
    val rddData2 = rddData1.map(row =&gt; {
      if (row.equals(&quot;s&quot;)) {
        row + &quot;s&quot;
      } else {
        row
      }
    })
    println(&quot;求并集:&quot;)
    rddData1.union(rddData2).foreach(println(_))
    println(&quot;求交集:&quot;)
    rddData1.intersection(rddData2).foreach(println(_))

    val rddData3 = rddData1.union(rddData2)
    println(&quot;去重:&quot;)
    rddData3.distinct().collect().foreach(println(_))
  }
</code></pre>
<ul>
<li>
<p><strong>intersection(otherDataset)</strong>  对源RDD和参数中的RDD求交集后返回一个新的RDD；</p>
</li>
<li>
<p><strong>distinct([numTasks]))</strong>  对源RDD进行去重后返回一个新的RDD；</p>
</li>
<li>
<p><strong>groupByKey([numTasks])</strong>  在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD；</p>
</li>
</ul>
<pre><code class="language-scala">  /**
    * 分组示例
    */
  def groupByKeyDemo(): Unit = {
    val sparkSession = getDefaultSparkSession
    val textFileRDD = sparkSession.sparkContext.textFile(&quot;sparkRDD/src/main/resources/data.txt&quot;)
    val rddData = textFileRDD.flatMap(_.split(&quot; &quot;)).map(row =&gt; (row, 1))
    rddData.groupByKey().map(row =&gt; {
      val count = row._2.sum
      (row._1, count)
    }).collect().foreach(println(_))
    rddData.reduceByKey((x, y) =&gt; x + y).collect().foreach(println(_))
    //    rddData.groupByKey().collect().foreach(println(_))
  }

</code></pre>
<ul>
<li><strong>reduceByKey(func, [numTasks])</strong> 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置； 与groupByKey的不同在于reduceByKey中可以传入一个函数，处理规约后的每个值；groupByKey则是将分组后的值都放到Iterator中；</li>
</ul>
<pre><code class="language-scala">  val textFileRDD = sparkSession.sparkContext.textFile(&quot;sparkRDD/src/main/resources/data.txt&quot;)
      val rddData = textFileRDD.flatMap(_.split(&quot; &quot;)).map(row =&gt; (row, 1))
      rddData.groupByKey().map(row =&gt; {
        val count = row._2.sum
        (row._1, count)
      }).collect().foreach(println(_))
  
      rddData.reduceByKey((x, y) =&gt; x + y).collect().foreach(println(_))
</code></pre>
<p>看完reduceByKey之后再去看看distinct()的源码，就会发现很有意思：</p>
<pre><code class="language-scala">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)
  }
</code></pre>
<p>先是将一个RDD转化为(x,null) 这种二元结构，然后按照每个key进行规约，这样就能保证key只有一个，而x,y都为null，最后只需要再将规约后的key取出来，就是去重后的RDD了。</p>
<ul>
<li><strong>aggregateByKey (zeroValue)(seqOp, combOp, [numTasks])</strong>  先按分区聚合 ，再总的聚合 ；每次要跟初始值交流 例如：aggregateByKey(0)(<em>+</em>,<em>+</em>) 对k/y的RDD进行操作；</li>
</ul>
<pre><code class="language-scala">  /**
    * 聚合，暂时还没理解
    */
  def aggregateByKeyDemo(): Unit = {
    val sparkSession = getDefaultSparkSession
    val textFileRDD = sparkSession.sparkContext.textFile(&quot;sparkRDD/src/main/resources/data.txt&quot;)
    val rddData = textFileRDD.flatMap(_.split(&quot; &quot;)).map(row =&gt; (row, 1))
    val aggregateRDD = rddData.aggregateByKey(0)(_ + _, _ + _)
    aggregateRDD.collect().foreach(println(_))
  }
</code></pre>
<ul>
<li><strong>sortByKey([<em>ascending</em>], [<em>numPartitions</em>])</strong>  在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD；</li>
</ul>
<pre><code class="language-scala">  /**
    * 特殊的排序
    */
  def sortByKeyDemo(): Unit = {
    val sparkSession = getDefaultSparkSession
    val rddData2 = sparkSession.sparkContext.parallelize(List(&quot;CSDN&quot;, &quot;ITEYE&quot;, &quot;CNBLOG&quot;, &quot;OSCHINA&quot;, &quot;GITHUB&quot;))
    val rddData3 = sparkSession.sparkContext.parallelize(1 to rddData2.count().toInt)
    val rddData4 = rddData2.zip(rddData3)
    rddData4.sortByKey().collect().foreach(println(_))
  }
</code></pre>
<ul>
<li><strong>sortBy(func,[ascending], [numTasks])</strong> 与sortByKey类似，排序的对象也是（K,V）结构，第一个参数是根据什么排序， 第二个是怎么排序 false倒序 ，第三个排序后分区数 ，默认与原RDD一样；</li>
</ul>
<pre><code class="language-scala">  def sortByDemo(): Unit = {
      val sparkSession = getDefaultSparkSession
      val rddData = sparkSession.sparkContext.parallelize(List(3, 23, 4, 6, 234, 87))
      val newRdd = rddData.mapPartitionsWithIndex((index: Int, row: Iterator[Int]) =&gt; {
        row.toList.map(r =&gt; (index, r)).iterator
      })
      newRdd.sortBy(_._2, ascending = false).collect().foreach(println(_))
    }
</code></pre>
<ul>
<li><strong>join(otherDataset, [numTasks])</strong>  在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD ,相当于内连接（求交集)；</li>
</ul>
<pre><code class="language-scala">  /**
    * zip 和 join 操作
    */
  def joinDemo(): Unit = {
    val sparkSession = getDefaultSparkSession
    val rddDataName = sparkSession.sparkContext.parallelize(List(&quot;Tom&quot;, &quot;Rose&quot;, &quot;Jack&quot;, &quot;Jerry&quot;))
    val rddDataId = sparkSession.sparkContext.parallelize(List(&quot;1001&quot;, &quot;1002&quot;, &quot;1003&quot;, &quot;1004&quot;))
    val rddDataAge = sparkSession.sparkContext.parallelize(List(12, 22, 13, 20))
    val rddIdAndName = rddDataId.zip(rddDataName)
    rddIdAndName.collect().foreach(println(_))
    val rddIdAndAge = rddDataId.zip(rddDataAge)
    rddIdAndAge.collect().foreach(println(_))
    val fullRdd = rddIdAndName.join(rddIdAndAge)
    fullRdd.collect().foreach(println(_))
  }
</code></pre>
<ul>
<li><strong>cogroup(otherDataset, [numTasks])</strong>  在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD；在2.3之后没有该方法。</li>
</ul>
<h4 id="小细节1-引用成员变量">小细节1 引用成员变量</h4>
<p>以下代码中map中引用了class中的成员变量；</p>
<pre><code class="language-scala">class MyClass {
  val field = &quot;Hello&quot;
  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(x =&gt; field + x) }
}
</code></pre>
<p>但这种方式等价于： <code>scala rdd.map(x =&gt; this.field + x) </code> ，这种情况会引用整个this；
正确的做法是这样的：</p>
<pre><code class="language-scala">def doStuff(rdd: RDD[String]): RDD[String] = {
  // 复制一份副本到本地
  val field_ = this.field
  rdd.map(x =&gt; field_ + x)
}
</code></pre>
<p>在map中引用成员变量，应该在进行转换之前就复制一份副本到本地，然后使用本地的副本而不是去引用成员变量；</p>
<h4 id="小细节2-求和操作">小细节2 求和操作</h4>
<p>不能在代码中直接使用foreach求和</p>
<pre><code class="language-scala">var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x =&gt; counter += x)

println(&quot;Counter value: &quot; + counter)
</code></pre>

</div>
</main>

<section class="appendix">





<div>
  <div class="side side-left"><h3>重复使用</h3></div>
  Text and figures are licensed under <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution CC BY 4.0</a>. The source code is licensed under MIT. The full source is available at <a href="https://github.com/yihui/hugo-prose">https://github.com/yihui/hugo-prose</a>.
</div>



<div>
  <div class="side side-left"><h3>欢迎修订</h3></div>
  
  
  
    
    
  
  如果您发现本文里含有任何错误（包括错别字和标点符号），欢迎<a href="https://github.com/yihui/hugo-prose/edit/master/exampleSite/content/post/2019-7-27-spark%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-RDD%e5%9f%ba%e7%a1%80%e7%ae%97%e5%ad%90.md" id="edit-link">在本站的 GitHub 项目里提交修订意见。</a>
</div>




</section>



<nav class="post-nav">
  <span class="nav-next">&larr; <a href="/post/2019/07/11/springboot-hbase/" title=下一篇&#32;(旧)>SpringBoot HBase</a></span>
  &hercon;
  <span class="nav-prev"><a href="/post/2019/09/04/%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%AE%97%E6%B3%95/" title=上一篇&#32;(新)>逆波兰表达式算法</a> &rarr;</span>
</nav>





</div>


  <footer>
  






<script src="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/js/number-sections.min.js,npm/@xiee/utils/js/toc.min.js,npm/@xiee/utils/js/toc-highlight.min.js,npm/@xiee/utils/js/sidenotes.min.js,npm/@xiee/utils/js/right-quote.min.js,npm/@xiee/utils/js/center-img.min.js,npm/@xiee/utils/js/fix-pandoc.min.js,npm/@xiee/utils/js/heading-anchor.min.js" defer></script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/rstudio/markdown/inst/resources/prism-xcode.css">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>


  <div class="footer">
  
  <ul class="copyright">
    <li>© <a href="https://en.wikipedia.org/wiki/Seneca_the_Younger">Lucius Annaeus Seneca</a> 4 BC &ndash; AD 65</li>
  </ul>
  
  <ul>
    
    <li>
      <a href="/404.html">Contact</a>
    </li>
    
    <li class="optional">
      <a href="/categories/">Categories</a>
    </li>
    
    <li class="optional">
      <a href="/tags/">Tags</a>
    </li>
    
    <li id="menu-edit">
      <a href="#">Suggest an edit</a>
    </li>
    
    <li>
      <a href="#">Back to top</a>
    </li>
    
  </ul>
  </div>
  
  </footer>
  <script src="/js/features.js" defer></script>
  </body>
</html>

